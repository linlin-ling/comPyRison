---
title: "reddit_python_analysis"
author: "Yulin Chang"
date: "2019年12月16日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm)
reddit_python <- read_csv("./data/reddit_python_processed.csv")
# view data
reddit_python[c(1,10,100,1000,10000), ]
```


```{r}
# DO NOT RUN THIS CHUNK
keeps <- c("author", "created_utc", "id", "url", "num_comments", "selftext", "title")
# keeps only useful data 
reddit_python <- reddit_python[keeps] %>% 
  group_by(id) %>% 
  # compute length and number of question marks per post
mutate(length = length(unlist(strsplit(selftext, " "))), q_marks = length(unlist(str_match_all(selftext, "\\?"))), politeness = length(unlist(str_match_all(selftext, "[Pp]lease|[Tt]hx|[Tt]hanks?|PLEASE|[Pp]lz|PLZ|THX|THANKS|THANK"))))

# compute total post published by every author
reddit_python_10kings <- reddit_python %>% 
  group_by(author) %>% 
  summarise(total_post = n()) %>% 
  filter(author != "[deleted]") %>% 
  arrange(desc(total_post)) %>% 
  top_n(10)
# get top 10 authors with most posts
reddit_kings <- ggplot(reddit_python_10kings, aes(x = author, y = total_post))+
  geom_bar(stat = "identity")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  geom_text(aes(label=total_post), vjust=-0.25)
reddit_kings

```

```{r}

```

```{r}
library(tidytext)
data("stop_words")

# remove unwanted characters in title
reddit_python$title <- str_replace(reddit_python$title, "[\\?-]", "")
# build tidytext format for title
tidy_text_title <- reddit_python %>%
  unnest_tokens(output = "word", input = "title",
                token = "regex", pattern = " ") %>% 
  # removing stop words in content
  anti_join(stop_words, by = c("word" = "word"))
# the frequency list of title
freq_word_title <- tidy_text_title %>%
  count(word) %>%
  arrange(desc(n))
# frequency plot
ggplot(freq_word_title[1:10,], aes(x = reorder(word, -n), y = n))+
  geom_bar(stat = "identity", fill = "#FEB2A2")+
  coord_cartesian(ylim = c(0, 45000))+
  geom_text(aes(label=n), vjust = 0.25)+
  coord_flip()+
  ylab("Top 10 Words used in titles of REDDIT/Python")+
  xlab("Word")
# wordcloud for title
library(wordcloud2)
wordcloud2(freq_word_title, fontFamily = "Gen Jyuu Gothic Medium")

# build tidytext format for content
tidy_text_content <- reddit_python %>% 
  filter(!(is.na(selftext))) %>%
  unnest_tokens(output = "word", input = "selftext",
                token = "regex", pattern = " ") %>% 
  # removing stop words in content
  anti_join(stop_words, by = c("word" = "word"))
# the frequency list of content
freq_word_content <- tidy_text_content %>%
  count(word) %>%
  arrange(desc(n))
freq_word_content <- freq_word_content[!(freq_word_content$word %in% c("\r\r\r\n", "[removed]", "[deleted]")), ]
# frequency plot
ggplot(freq_word_content[1:20,], aes(x = reorder(word, -n), y = n))+
  geom_bar(stat = "identity")+
  geom_text(aes(label=n), vjust=-0.25)+
  xlab("Top 20 Words used in post contents of REDDIT/Python")
```

```{r}
# match errors in content
error <- str_match(tidy_text_content$word,"([^-_\\[\\^\\\\&g;!\'\"\\n\\*`\\(\\t#].+error):")
# build a dataframe for errors
df_error <- data.frame(error_type = error[,2][!is.na(error)])
# count the frequency for every error
df_error <- df_error %>% group_by(error_type) %>% summarise(count=n()) %>% arrange(desc(count)) 
# plot error frequency
ggplot(df_error[2:11,], aes(x = reorder(error_type, -count), y = count))+
  geom_bar(stat = "identity", fill = "#FFBE75")+
  geom_text(aes(label=count))+
  coord_flip()+
  ylab("n")+
  xlab("errors")
```

