```{r}
library(dplyr)
library(stringr)
library(readr)
comment_python <- read.csv("comment_python.csv", encoding = "big5",stringsAsFactors = 0)
info_python <- read.csv("info_python.csv", encoding = "big5", stringsAsFactors = 0)

```

```{r}
#comment_python add three colums 1. number of comments . total comment
comment_python <- comment_python %>%
  group_by(post_id, push_date_time) %>%
  mutate(n_comments = n()) %>%
  group_by(post_id) %>%
  summarise(total_comments = sum(n_comments))
```


```{r}
info_python <- info_python %>%
   mutate(post_id = as.character(info_python$post_id)) %>%
   arrange(post_id)%>%
   mutate(total_comments = 0) 

for(i in seq(length(comment_python$post_id))){
  info_python$total_comments[which(info_python$post_id==comment_python$post_id[i])]=comment_python$total_comments[i]
}

```


```{r}
#info_python add 7 colums post_error、ttl_error、humble、polite、questions、n_char、BBS king
info_python <- info_python %>%
      mutate(polite = str_count(info_python$post_content,
                                "(請(問)?|謝謝|感(激|謝|恩)(不盡)?|(謝)?大大|不好意思)"))%>%
      mutate(humble = str_count(info_python$post_content, "((不才)?在下|小(弟|妹)|(小|本)魯|(魯蛇)(肥宅)?|(魯蛇)?(肥宅)|本肥(宅)?|前輩|新手)"))%>%
      mutate(question = str_count(info_python$post_content, "(\\?|\\？)")) %>%
      mutate(word_count = nchar(info_python$post_content, 
                                 type ="chars", allowNA = FALSE, keepNA = NA))%>%
      group_by(post_author) %>%
      mutate(bbs_king = n()) 
      
```

```{r}
#title segementation
library(jiebaR)

seg <- worker(user = "dict.txt")
docs_segged <- rep("", 4690)
for (i in seq_along(info_python$post_title)) {
      segged <- segment(info_python$post_title[i], seg)
      docs_segged[i] <- paste0(segged, collapse = " ")
}
seg_title <- tibble::tibble(seg_title = docs_segged)
```

```{r}
#title frequency
library(tidytext)

stopwords_tra <- read_csv("stopwords_tra.txt", 
    col_names = FALSE)

tidy_text_format1 <- seg_title %>%
  unnest_tokens(output = "word", input = "seg_title",
                token = "regex", pattern = " ") %>%
  anti_join(stopwords_tra, by = c("word" = "X1")) 

tidy_text_format1 <- tidy_text_format1 %>%
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
```


```{r}
#content segementation
library(jiebaR)

seg <- worker()
docs_segged <- rep("", 4690)
for (i in seq_along(info_python$post_content)) {
      segged <- segment(info_python$post_content[i], seg)
      docs_segged[i] <- paste0(segged, collapse = " ")
}
seg_content <- tibble::tibble(seg_content = docs_segged)
```


```{r}
#content frequency
library(tidytext)

stopwords_tra <- read_csv("stopwords_tra.txt", 
    col_names = FALSE)

tidy_text_format <- seg_content %>%
  unnest_tokens(output = "word", input = "seg_content",
                token = "regex", pattern = " ") %>%
  anti_join(stopwords_tra, by = c("word" = "X1")) 

tidy_text_format %>%
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
```

```{r}
tidy_text_format <- tidy_text_format %>%
  mutate(error = str_match(tidy_text_format$word, ".+error"))%>%
  group_by(error) %>%
  summarise(n = n())%>%
  arrange(desc(n)) %>%
  filter(n >20 )
tidy_text_format <-na.omit(tidy_text_format)
```



```{r}
#error code BARCHART
library(ggplot2)
ggplot(tidy_text_format) +
   geom_bar(aes(reorder(error,-n ),  n), stat = "identity", fill = "#46A3FF" )+
   labs(x = "errors",
        y = "n")+
   geom_text(aes(x = reorder(error, -n), y = n, label = n, vjust=-0.25))+
   theme(axis.text.x = element_text(size = 13),
     plot.title = element_text(hjust = 0.5)) +
       coord_flip()
```

```{r}
library(wordcloud2)
wordcloud2(tidy_text_format1)#, fontFamily = "Gen Jyuu Gothic Medium")
```

